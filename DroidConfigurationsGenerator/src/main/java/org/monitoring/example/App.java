/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package org.monitoring.example;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.ParsedSchema;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import org.apache.avro.Schema;

import java.io.IOException;
import java.io.OutputStream;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLConnection;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;

public class App {
    public static void main(String[] args) {
        // Getting all schemas from schema registry
        CachedSchemaRegistryClient schemaRegistryClient = new CachedSchemaRegistryClient("http://192.168.1.40:8087", 1000);

        try {
            Collection<String> subjects = schemaRegistryClient.getAllSubjects();
            HashMap<VersionedSchema, Schema> versionedSchemaSchemaMapper = new HashMap<>();
            subjects.forEach(subject -> {
                try {
                    Collection<Integer> versions = schemaRegistryClient.getAllVersions(subject);
                    versions.forEach(id -> {
                        VersionedSchema versionedSchema = new VersionedSchema(id, subject);
                        if (!versionedSchemaSchemaMapper.containsKey(versionedSchema)) {
                            try {
                                ParsedSchema parsedSchema = schemaRegistryClient.getSchemaBySubjectAndId(subject, id);
                                Schema.Parser schemaParser = new Schema.Parser();
                                Schema avroSchema = schemaParser.parse(parsedSchema.canonicalString());
                                versionedSchemaSchemaMapper.put(versionedSchema, avroSchema);
                            } catch (IOException | RestClientException e) {
                                e.printStackTrace();
                            }
                        }
                    });
                } catch (IOException | RestClientException e) {
                    e.printStackTrace();
                }
            });
            versionedSchemaSchemaMapper.forEach((versionedSchema, parsedSchema) -> {

                String dataSource = "BG_" + versionedSchema.getTopicName();
                TimestampSpec timestampSpec = new TimestampSpec("id.sendTimeMillis", "millis");
                DimensionsSpec dimensionsSpec = new DimensionsSpec();
                GranularitySpec granularitySpec = new GranularitySpec("hour", true, "hour");

                DataSchema dataSchema = new DataSchema(dataSource, timestampSpec, dimensionsSpec, granularitySpec);
                String topic = versionedSchema.getTopicName();
                String inputFormatType = "avro_stream";
                AvroBytesDecoder avroBytesDecoder = new AvroBytesDecoder("schema_registry", "http://192.168.1.40:8087");
                InputFormat inputFormat = new InputFormat(inputFormatType, avroBytesDecoder, parseFlattenSpec(parsedSchema));
                ConsumerProperties consumerProperties = new ConsumerProperties("192.168.1.40:9092");
                int replicas = 1;
                int taskCount = 1;
                String taskDuration = "PT1H";
                String type = "kafka";
                boolean useEarliestOffset = true;

                ioConfig ioConfig = new ioConfig(topic, inputFormat, consumerProperties, replicas, taskCount, taskDuration, type, useEarliestOffset);
                TuningConfig tuningConfig = new TuningConfig("kafka", 5000000);
                DruidRequestSpec spec = new DruidRequestSpec(dataSchema, ioConfig, tuningConfig);
                DruidSpec druidSpec = new DruidSpec("kafka", spec);
                ObjectMapper objectMapper = new ObjectMapper();
                String json = null;
                try {
                    json = objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(druidSpec);
                } catch (JsonProcessingException e) {
                    e.printStackTrace();
                }
                System.out.println(json);

                byte[] out = json.getBytes(StandardCharsets.UTF_8);
                int length = out.length;
                URL url = null;
                try {
                    url = new URL("http://localhost:8888/druid/indexer/v1/supervisor");
                    URLConnection con = url.openConnection();
                    HttpURLConnection http = (HttpURLConnection) con;
                    http.setRequestMethod("POST"); // PUT is another valid option
                    http.setDoOutput(true);
                    http.setFixedLengthStreamingMode(length);
                    http.setRequestProperty("Content-Type", "application/json; charset=UTF-8");
                    http.connect();
                    try (OutputStream os = http.getOutputStream()) {
                        os.write(out);
                    }
                } catch (IOException e) {
                    e.printStackTrace();
                }
            });
        } catch (IOException | RestClientException ignored) {
            ignored.printStackTrace();
        }
    }

    private static FlattenSpec parseFlattenSpec(Schema schema) {
        ArrayList<Field> fields = new ArrayList<>();
        AvroFlattener avroFlattener = new AvroFlattener();
        Schema dest = avroFlattener.flatten(schema, true);

        dest.getFields().stream().forEach(field -> {
            String fieldName = field.name();
            fieldName = fieldName.replaceAll("__", ".");
            fields.add(new Field(fieldName, "path", "$." + fieldName));
        });
        return new FlattenSpec(fields);
    }
}
